{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import List, Optional, Literal, Dict, Any\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import httpx\n",
    "#from rapidfuzz import process\n",
    "#from psycopg_pool import AsyncConnectionPool\n",
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import logging\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, CSVLoader\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage, merge_message_runs\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "#from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "#from langgraph.store.postgres.aio import AsyncPostgresStore\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader, WebBaseLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from trustcall import create_extractor\n",
    "# from model_loader import load_model\n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47907b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5359b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSG_PROMPT= \"\"\"\n",
    "You are a whatsapp assistant, your duty is to assist business owners attend to customers.\n",
    "\n",
    "Your primary responsibilities:\n",
    "- Provide excellent customer service with a warm, conversational tone\n",
    "- Help customers find information about menu items, pricing, and availability\n",
    "- Remember customer preferences and delivery addresses for personalized service\n",
    "- Guide customers through the complete order process including address collection\n",
    "- Handle common questions efficiently while knowing when to escalate complex issues\n",
    "- Add items to cart and always ask customer if they want to add to their order.\n",
    "- When customers are ready to pay or complete their order, check for their address and confirm if they want pick-up or delivery.\n",
    "- If user wants delivery, go through prior conversations to check for address, if no address provided during conversation, ask user for delivery address \n",
    "\n",
    "Customer Profile:\n",
    "<user_profile>\n",
    "{user_profile}\n",
    "</user_profile>\n",
    "\"\"\"\n",
    "\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "   - Delivery address\n",
    "   - Ordered items\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, structured profile\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "CRITICAL: \n",
    "- Use null (not the string \"None\") for fields with no information\n",
    "- Only include factual information directly stated by the user\n",
    "- Do not make assumptions or inferences\n",
    "- Leave fields as null if not mentioned in the conversation\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profile(BaseModel):\n",
    "    \"\"\"User profile information for personalizing customer service.\n",
    "    \n",
    "    IMPORTANT: Use null for unknown fields, not the string \"None\".\n",
    "    Only populate fields with actual information from the conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Customer's name. Use null if not mentioned.\"\n",
    "    )\n",
    "    location: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Customer's general location or city. Use null if not mentioned.\"\n",
    "    )\n",
    "    address: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Customer's full delivery address. Use null if not mentioned.\"\n",
    "    )\n",
    "    items: Optional[List[str]] = Field(\n",
    "        default=None, \n",
    "        description=\"List of items in customer's order. Use null if no items ordered yet.\"\n",
    "    )\n",
    "    human_active: Optional[bool] = Field(\n",
    "        default=None, \n",
    "        description=\"Whether a human agent is currently handling this customer. Use null if unknown.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Profile extractor\n",
    "# -----------------------------\n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\",\n",
    ")\n",
    "\n",
    "\n",
    "# Update memory tool\n",
    "class CustomerAction(BaseModel):\n",
    "    \"\"\"Handle customer requests which may include multiple actions.\n",
    "    \n",
    "    This tool should be called whenever the customer:\n",
    "    - Shares personal information (name, address, preferences)\n",
    "    - Asks about menu items or availability\n",
    "    - Wants to place an order\n",
    "    \"\"\"\n",
    "    \n",
    "    update_profile: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if message contains profile information to save (name, location, address, preferences)\"\n",
    "    )\n",
    "    \n",
    "    search_menu: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer is asking about menu items, prices, or availability\"\n",
    "    )\n",
    "    \n",
    "    search_query: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"The specific menu item, category, or question they're asking about\"\n",
    "    )\n",
    "    \n",
    "    ready_to_order: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer is ready to place/finalize their order\"\n",
    "    )\n",
    "\n",
    "    ready_to_pay: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer is ready to pay for their order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5574fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"profile\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # Extract the actual memory content if it exists\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MSG_PROMPT.format(user_profile=existing_memory_content)\n",
    "    \n",
    "    # FIXED: Pass both system message AND conversation history\n",
    "    response = model.bind_tools([CustomerAction]).invoke(\n",
    "        [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# No extract_and_save_profile function at all!\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Extract and save profile information (all-in-one).\"\"\"\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"profile\", user_id)\n",
    "    \n",
    "    existing_items = store.search(namespace)\n",
    "    tool_name = \"Profile\"\n",
    "    existing_memories = (\n",
    "        [(item.key, tool_name, item.value) for item in existing_items]\n",
    "        if existing_items else None\n",
    "    )\n",
    "    \n",
    "    conversation_history = [\n",
    "        msg for msg in state[\"messages\"]\n",
    "        if not (hasattr(msg, 'type') and msg.type == 'tool')\n",
    "    ]\n",
    "    \n",
    "    TRUSTCALL_INSTRUCTION_FORMATTED = TRUSTCALL_INSTRUCTION.format(memory=existing_memories)\n",
    "    updated_messages = list(merge_message_runs(\n",
    "        [SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + conversation_history\n",
    "    ))\n",
    "    \n",
    "    result = profile_extractor.invoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "    \n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(\n",
    "            namespace,\n",
    "            rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "    \n",
    "    # Return LangGraph message directly (no helper function)\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Profile information saved successfully\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# KB1 Setup\n",
    "KB1_DOC_PATH = r\"C:\\Users\\HP\\Desktop\\WAHA\\synthetic_restaurant_menu_10000.csv\"\n",
    "\n",
    "# Module-level cache (outside functions)\n",
    "_vector_store_cache = None\n",
    "_retriever_cache = None\n",
    "_file_hash_cache = None\n",
    "\n",
    "\n",
    "def get_file_hash(path):\n",
    "    \"\"\"Compute a hash of the file contents to detect changes.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        hasher.update(f.read())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "def initialize_rag(doc_path=None):\n",
    "    \"\"\"Initialize RAG system with documents. Call once at startup.\"\"\"\n",
    "    global _vector_store_cache, _retriever_cache, _file_hash_cache\n",
    "    \n",
    "    # Check if already initialized\n",
    "    if _vector_store_cache is not None:\n",
    "        print(\"RAG already initialized\")\n",
    "        return _retriever_cache\n",
    "    \n",
    "    document_path = doc_path or KB1_DOC_PATH\n",
    "    persist_dir = \"./chroma_rag_KB1\"\n",
    "\n",
    "    try:\n",
    "        _embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL_NAME,\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        # Check if persisted DB exists\n",
    "        if os.path.exists(persist_dir) and os.path.exists(document_path):\n",
    "            current_hash = get_file_hash(document_path)\n",
    "            \n",
    "            # Try to load existing DB\n",
    "            try:\n",
    "                print(\"Loading existing knowledge base...\")\n",
    "                _vector_store_cache = Chroma(\n",
    "                    collection_name=\"kb1_collection\",  # Use fixed name\n",
    "                    embedding_function=_embeddings,\n",
    "                    persist_directory=persist_dir\n",
    "                )\n",
    "                _file_hash_cache = current_hash\n",
    "                _retriever_cache = _vector_store_cache.as_retriever(search_kwargs={\"k\": 5})\n",
    "                print(\"âœ… Loaded existing knowledge base (fast load)\")\n",
    "                return _retriever_cache\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load existing DB: {e}. Rebuilding...\")\n",
    "        \n",
    "        # If no existing DB or load failed, build from scratch\n",
    "        print(\"Building knowledge base from scratch...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "        all_documents = []\n",
    "        \n",
    "        # Load from document\n",
    "        if document_path and os.path.exists(document_path):\n",
    "            print(f\"Processing {document_path}\")\n",
    "            if document_path.lower().endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(document_path)\n",
    "            elif document_path.lower().endswith(\".docx\"):\n",
    "                loader = Docx2txtLoader(document_path)\n",
    "            elif document_path.lower().endswith(\".csv\"):\n",
    "                loader = CSVLoader(file_path=document_path, encoding=\"utf-8-sig\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {document_path}\")\n",
    "            \n",
    "            docs = loader.load()\n",
    "            doc_chunks = splitter.split_documents(docs)\n",
    "            all_documents.extend(doc_chunks)\n",
    "            print(f\"Loaded {len(doc_chunks)} chunks from {document_path}\")\n",
    "\n",
    "        if not all_documents:\n",
    "            raise ValueError(\"No documents were loaded\")\n",
    "\n",
    "        # Store file hash\n",
    "        _file_hash_cache = get_file_hash(document_path)\n",
    "        \n",
    "        print(f\"Creating knowledge base with {len(all_documents)} document chunks\")\n",
    "        _vector_store_cache = Chroma.from_documents(\n",
    "            documents=all_documents,\n",
    "            collection_name=\"kb1_collection\", \n",
    "            embedding=_embeddings,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "        \n",
    "        _retriever_cache = _vector_store_cache.as_retriever(search_kwargs={\"k\": 5})\n",
    "        print(f\"âœ… Setup complete\")\n",
    "        \n",
    "        return _retriever_cache\n",
    "      \n",
    "    except Exception as e:\n",
    "        logger.error(f\"RAG initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def refresh_rag(doc_path=None):\n",
    "    \"\"\"Refresh RAG when file changes detected.\"\"\"\n",
    "    global _vector_store_cache, _retriever_cache, _file_hash_cache\n",
    "    \n",
    "    print(\"ðŸ”„ Refreshing knowledge base...\")\n",
    "    \n",
    "    # Clear cache\n",
    "    _vector_store_cache = None\n",
    "    _retriever_cache = None\n",
    "    _file_hash_cache = None\n",
    "    \n",
    "    # Reinitialize\n",
    "    retriever = initialize_rag(doc_path)\n",
    "    print(\"âœ… Knowledge base refreshed!\")\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "# File change handler\n",
    "class KBUpdateHandler(FileSystemEventHandler):\n",
    "    def on_modified(self, event):\n",
    "        if event.src_path == os.path.abspath(KB1_DOC_PATH):\n",
    "            print(f\"ðŸ“ Detected change in file: {event.src_path}\")\n",
    "            try:\n",
    "                refresh_rag()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error refreshing: {e}\")\n",
    "\n",
    "\n",
    "def start_file_monitoring():\n",
    "    \"\"\"Start watching file for changes.\"\"\"\n",
    "    event_handler = KBUpdateHandler()\n",
    "    observer = Observer()\n",
    "    \n",
    "    kb_dir = os.path.dirname(os.path.abspath(KB1_DOC_PATH))\n",
    "    observer.schedule(event_handler, kb_dir, recursive=False)\n",
    "    \n",
    "    observer.start()\n",
    "    print(\"ðŸ‘€ File monitoring started...\")\n",
    "    return observer\n",
    "\n",
    "\n",
    "# Initialize at startup\n",
    "print(\"Initializing Knowledge Base...\")\n",
    "try:\n",
    "    initialize_rag()\n",
    "    print(\"Knowledge Base ready!\")\n",
    "    observer = start_file_monitoring()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Initialization failed: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "def rag_search(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Perform RAG search on knowledge base.\"\"\"\n",
    "    global _retriever_cache\n",
    "    \n",
    "    # Check if RAG is initialized\n",
    "    if _retriever_cache is None:\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Knowledge base not initialized. Please contact support.\",\n",
    "                \"tool_call_id\": state[\"messages\"][-1].tool_calls[0]['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call.get('args', {})\n",
    "    \n",
    "    search_query = args.get('search_query', '')\n",
    "    \n",
    "    if not search_query:\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"No search query provided.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # 1. Retrieve relevant documents from vector store\n",
    "        relevant_docs = _retriever_cache.invoke(search_query)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return {\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": \"I couldn't find any information about that in the knowledge base.\",\n",
    "                    \"tool_call_id\": tool_call['id']\n",
    "                }]\n",
    "            }\n",
    "        \n",
    "        # 2. Format context from retrieved docs\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # 3. Return the context (or you can generate a response using LLM)\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": context,\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"RAG search failed: {e}\")\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Sorry, I encountered an error searching the knowledge base. Please try again.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_address_and_finalize(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Check if user has address, ask if not, or prepare for escalation.\"\"\"\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"profile\", user_id)\n",
    "    \n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "    \n",
    "    # Check for address in profile\n",
    "    has_address = False\n",
    "    if existing_memory:\n",
    "        profile_data = existing_memory.value\n",
    "        has_address = profile_data.get('address') is not None\n",
    "    \n",
    "    if not has_address:\n",
    "        # Ask for address\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"User is ready to order but no delivery address on file. Please ask for their delivery address.\",\n",
    "                \"tool_call_id\": tool_calls[0]['id']\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        # Has address - ready to escalate\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"User has address on file and is ready to complete order. Escalating to operator.\",\n",
    "                \"tool_call_id\": tool_calls[0]['id']\n",
    "            }]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8498dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_customer_action(state: MessagesState) -> str:\n",
    "    \"\"\"Route to the appropriate action node based on priority.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if not (hasattr(last_message, 'tool_calls') and last_message.tool_calls):\n",
    "        return \"__end__\"\n",
    "    \n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call.get('args', {})\n",
    "    \n",
    "    # Simple priority: Process ONE action at a time\n",
    "    # Chatbot will handle the rest autonomously\n",
    "    \n",
    "    if args.get('ready_to_order'):\n",
    "        return \"check_address_and_finalize\"\n",
    "    \n",
    "    if args.get('update_profile'):\n",
    "        return \"write_memory\"\n",
    "    \n",
    "    if args.get('search_menu'):\n",
    "        return \"rag_search\"\n",
    "    \n",
    "    return \"__end__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_node(\"rag_search\", rag_search)\n",
    "builder.add_node(\"check_address_and_finalize\", check_address_and_finalize)\n",
    "\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_customer_action,\n",
    "    {\n",
    "        \"write_memory\": \"write_memory\",\n",
    "        \"rag_search\": \"rag_search\",\n",
    "        \"check_address_and_finalize\": \"check_address_and_finalize\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# All actions loop back to chatbot\n",
    "builder.add_edge(\"write_memory\", \"chatbot\")\n",
    "builder.add_edge(\"rag_search\", \"chatbot\")\n",
    "builder.add_edge(\"check_address_and_finalize\", \"chatbot\")\n",
    "\n",
    "# Store and compile\n",
    "across_thread_memory = InMemoryStore()\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile graph\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104612d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input to create a profile memory\n",
    "input_messages = [HumanMessage(content=\"My name is Lance. I live in SF with my wife. I have a 1 year old daughter.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ec722",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input to create a profile memory\n",
    "input_messages = [HumanMessage(content=\"My name is Lance. I live in SF with my wife. I have a 1 year old daughter.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd38d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input to create a profile memory\n",
    "input_messages = [HumanMessage(content=\"My name is Lance. I live in SF with my wife. I have a 1 year old daughter.\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f01eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"What do you recommend for breakfast?\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"I'm still waiting for you to get back on my question\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"Tell me more about the sandwich you have\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83286285",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"Do you have Tangy Beef Sandwich?\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"Which resturant has the cheapest?\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"Ok, i'd like to order it\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"I'd like to pick it up\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"When will my order be ready for pickup?\")]\n",
    "\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a4e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a641c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010d5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ff41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0834f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2ee026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_postgres\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PGVector\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_huggingface\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     ChatHuggingFace,  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     HuggingFaceEmbeddings,\n\u001b[0;32m      6\u001b[0m     HuggingFaceEndpointEmbeddings,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     HuggingFaceEndpoint,\n\u001b[0;32m     10\u001b[0m     HuggingFacePipeline,\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_huggingface\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     TGI_MESSAGE,\n\u001b[0;32m      3\u001b[0m     TGI_RESPONSE,\n\u001b[0;32m      4\u001b[0m     ChatHuggingFace,\n\u001b[0;32m      5\u001b[0m     _convert_dict_to_message,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTGI_MESSAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTGI_RESPONSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatHuggingFace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_dict_to_message\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Literal, Optional, Union, cast\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     AsyncCallbackManagerForLLMRun,\n\u001b[0;32m     14\u001b[0m     CallbackManagerForLLMRun,\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageModelInput\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     BaseChatModel,\n\u001b[0;32m     19\u001b[0m     agenerate_from_stream,\n\u001b[0;32m     20\u001b[0m     generate_from_stream,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     AIMessage,\n\u001b[0;32m     24\u001b[0m     AIMessageChunk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     ToolMessageChunk,\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py:112\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(attr_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    111\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m _dynamic_imports\u001b[38;5;241m.\u001b[39mget(attr_name)\n\u001b[1;32m--> 112\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mimport_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__spec__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[attr_name] \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_import_utils.py:36\u001b[0m, in \u001b[0;36mimport_attr\u001b[1;34m(attr_name, module_name, package)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     38\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\base.py:44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMResult\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     _HAS_TRANSFORMERS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     31\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     42\u001b[0m BASIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "\n",
    "# Fix for Windows + psycopg async - ONLY apply on Windows\n",
    "if sys.platform == \"win32\":\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "import uuid\n",
    "from typing import List, Optional, Literal, Dict, Any\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import httpx\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import logging\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "#from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, CSVLoader\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage, merge_message_runs\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader, WebBaseLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from typing import Any, Callable, Tuple\n",
    "from functools import wraps\n",
    "from openai import OpenAI, APIError, RateLimitError, Timeout\n",
    "from threading import Lock\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from trustcall import create_extractor\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API_KEY not found in environment variables. Please check your .env file\")\n",
    "\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "try:\n",
    "    model = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=API_KEY)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to initialize ChatGroq model. Check your API key: {e}\")\n",
    "\n",
    "\n",
    "def retry_with_backoff(\n",
    "        max_retries: int = 5,\n",
    "        initial_delay: float = 1.0,\n",
    "        exponential_base: float = 2.0,\n",
    "        jitter: str = \"full\",\n",
    "        exceptions: Tuple = (Exception,)\n",
    "):\n",
    "    \"\"\"\n",
    "    Retry decorator with exponential backoff and optional jitter.\n",
    "\n",
    "    Jitter modes:\n",
    "        - none: no jitter\n",
    "        - full: random(0, delay)\n",
    "        - equal: delay/2 + random(0, delay/2)\n",
    "        - decorrelated: random(initial_delay, delay * 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def add_jitter(delay: float) -> float:\n",
    "        if jitter == \"none\":\n",
    "            return delay\n",
    "        if jitter == \"full\":\n",
    "            return random.uniform(0, delay)\n",
    "        if jitter == \"equal\":\n",
    "            return delay / 2 + random.uniform(0, delay / 2)\n",
    "        if jitter == \"decorrelated\":\n",
    "            return random.uniform(initial_delay, delay * 3)\n",
    "        return delay\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs) -> Any:\n",
    "            delay = initial_delay\n",
    "            last_exception = None\n",
    "\n",
    "            for attempt in range(1, max_retries + 2):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except exceptions as e:\n",
    "                    last_exception = e\n",
    "\n",
    "                    if attempt > max_retries:\n",
    "                        logger.error(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                        raise\n",
    "\n",
    "                    sleep_time = add_jitter(delay)\n",
    "                    logger.warning(\n",
    "                        f\"Attempt {attempt} failed: {e}. \"\n",
    "                        f\"Retrying in {sleep_time:.2f}s\"\n",
    "                    )\n",
    "\n",
    "                    time.sleep(sleep_time)\n",
    "                    delay *= exponential_base\n",
    "\n",
    "            raise last_exception\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MSG_PROMPT= \"\"\"\n",
    "You are a whatsapp assistant, your duty is to assist business owners attend to customers.\n",
    "\n",
    "Your primary responsibilities:\n",
    "- Provide excellent customer service with a warm, conversational tone\n",
    "- Help customers find information about menu items, pricing, and availability\n",
    "- Remember customer preferences and delivery addresses for personalized service\n",
    "- Guide customers through the complete order process including address collection\n",
    "- Handle common questions efficiently while knowing when to escalate complex issues\n",
    "- Add items to cart and always ask customer if they want to add to their order.\n",
    "- When customers are ready to pay or complete their order, check for their address and confirm if they want pick-up or delivery.\n",
    "- If user wants delivery, go through prior conversations to check for address, if no address provided during conversation, ask user for delivery address \n",
    "\n",
    "Customer Profile:\n",
    "<user_profile>\n",
    "{user_profile}\n",
    "</user_profile>\n",
    "\"\"\"\n",
    "\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, address, location, cart)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "   - Delivery address\n",
    "   - Ordered items\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, structured profile\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "CRITICAL: \n",
    "- Use null (not the string \"None\") for fields with no information\n",
    "- Only include factual information directly stated by the user\n",
    "- Do not make assumptions or inferences\n",
    "- Leave fields as null if not mentioned in the conversation\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "\n",
    "class Profile(BaseModel):\n",
    "    \"\"\"User profile information for personalizing customer service.\n",
    "    \n",
    "    IMPORTANT: Use null for unknown fields, not the string \"None\".\n",
    "    Only populate fields with actual information from the conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    name: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Customer's name. Use null if not mentioned.\"\n",
    "    )\n",
    "    location: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Customer's general location or city. Use null if not mentioned.\"\n",
    "    )\n",
    "    address: Optional[str] = Field(\n",
    "        default=None, \n",
    "        description=\"Customer's full delivery address. Use null if not mentioned.\"\n",
    "    )\n",
    "    cart: Optional[List[str]] = Field(\n",
    "        default=None, \n",
    "        description=\"List of items in customer's order. Use null if no items ordered yet.\"\n",
    "    )\n",
    "    \n",
    "    human_active: Optional[bool] = Field(\n",
    "        default=None, \n",
    "        description=\"Whether a human agent is currently handling this customer. Use null if unknown.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Profile extractor\n",
    "# -----------------------------\n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\",\n",
    ")\n",
    "\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "# Update memory tool\n",
    "from typing import List, Union, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CartItem(BaseModel):\n",
    "    \"\"\"Cart item with quantity support\"\"\"\n",
    "    item: str = Field(description=\"Name of the product/item\")\n",
    "    quantity: int = Field(\n",
    "        default=1, \n",
    "        ge=1, \n",
    "        le=99, \n",
    "        description=\"Quantity to add (1-99, default: 1)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomerAction(BaseModel):\n",
    "    \"\"\"Handle customer requests which may include multiple actions.\n",
    "    \n",
    "    This tool should be called whenever the customer:\n",
    "    - Shares personal information (name, address, preferences)\n",
    "    - Asks about menu items or availability\n",
    "    - Wants to add/remove items from cart\n",
    "    - Wants to place an order\n",
    "    \"\"\"\n",
    "    \n",
    "    update_profile: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if message contains profile information to save (name, location, address, preferences) - NOT for cart items\"\n",
    "    )\n",
    "    \n",
    "    search_menu: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer is asking about menu items, prices, or availability without wanting to order yet\"\n",
    "    )\n",
    "    \n",
    "    search_query: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"The specific menu item, category, or question they're asking about\"\n",
    "    )\n",
    "    \n",
    "    add_to_cart: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer wants to add item(s) to their cart or order something\"\n",
    "    )\n",
    "    \n",
    "    # âœ… FIX: Use List[CartItem] instead of Union\n",
    "    cart_items: Optional[List[CartItem]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"List of items to add to cart with quantities. \"\n",
    "            \"Each item should have 'item' (name) and 'quantity' (number). \"\n",
    "            \"Example: [{'item': 'Burger', 'quantity': 2}, {'item': 'Fries', 'quantity': 1}]\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    remove_from_cart: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer wants to remove item(s) from cart\"\n",
    "    )\n",
    "    \n",
    "    # Use List[CartItem] instead of Union\n",
    "    items_to_remove: Optional[List[CartItem]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Items to remove from cart with quantities. \"\n",
    "            \"Each item should have 'item' (name) and 'quantity' (number to remove). \"\n",
    "            \"Example: [{'item': 'Burger', 'quantity': 1}]\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    view_cart: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer wants to see what's in their cart\"\n",
    "    )\n",
    "    \n",
    "    ready_to_order: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer is ready to place/finalize their order\"\n",
    "    )\n",
    "\n",
    "    ready_to_pay: bool = Field(\n",
    "        default=False,\n",
    "        description=\"True if customer is ready to pay for their order\"\n",
    "    )\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MAX_MEMORY_MESSAGES = 100\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "from psycopg_pool import AsyncConnectionPool\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "from langgraph.store.postgres.aio import AsyncPostgresStore\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Use DIRECT connection for LangGraph (checkpointer/store)\n",
    "POSTGRES_URI = os.getenv(\"POSTGRES_URI\")\n",
    "# Use POOLER connection for PGVector\n",
    "POSTGRES_URI_POOLER = os.getenv(\"POSTGRES_URI_POOLER\")\n",
    "\n",
    "if not POSTGRES_URI:\n",
    "    raise ValueError(\"POSTGRES_URI not set\")\n",
    "if not POSTGRES_URI_POOLER:\n",
    "    raise ValueError(\"POSTGRES_URI_POOLER not set\")\n",
    "\n",
    "\n",
    "# After all imports and before any functions\n",
    "store = None\n",
    "saver = None\n",
    "\n",
    "# Make sure this is BEFORE the setup_database function\n",
    "# -----------------------------\n",
    "# Connect to DB\n",
    "# -----------------------------\n",
    "\n",
    "_pool: AsyncConnectionPool = None\n",
    "\n",
    "async def drop_old_tables_once():\n",
    "    \"\"\"Drop old tables - run once then comment out\"\"\"\n",
    "    import asyncpg\n",
    "    \n",
    "    print(\"Dropping old tables...\")\n",
    "    clean_uri = POSTGRES_URI.split('?')[0]\n",
    "    conn = await asyncpg.connect(clean_uri, timeout=30)\n",
    "    \n",
    "    try:\n",
    "        await conn.execute(\"DROP TABLE IF EXISTS store CASCADE;\")\n",
    "        await conn.execute(\"DROP TABLE IF EXISTS checkpoints CASCADE;\")\n",
    "        await conn.execute(\"DROP TABLE IF EXISTS writes CASCADE;\")\n",
    "        print(\"âœ… Old tables dropped\")\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "async def create_tables_manually():\n",
    "    \"\"\"Create tables using the connection pool instead of direct connection\"\"\"\n",
    "    print(\"Creating tables manually via connection pool...\")\n",
    "    \n",
    "    pool = await get_pool()\n",
    "    \n",
    "    async with pool.connection() as conn:\n",
    "        # Create store table\n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS store (\n",
    "                prefix TEXT NOT NULL,\n",
    "                key TEXT NOT NULL,\n",
    "                value JSONB NOT NULL,\n",
    "                created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
    "                updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
    "                ttl_minutes INTEGER,\n",
    "                expires_at TIMESTAMP WITH TIME ZONE,\n",
    "                PRIMARY KEY (prefix, key)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes\n",
    "        await conn.execute(\"CREATE INDEX IF NOT EXISTS store_prefix_key_idx ON store(prefix, key);\")\n",
    "        await conn.execute(\"CREATE INDEX IF NOT EXISTS store_prefix_idx ON store(prefix);\")\n",
    "        await conn.execute(\"CREATE INDEX IF NOT EXISTS store_expires_at_idx ON store(expires_at) WHERE expires_at IS NOT NULL;\")\n",
    "        print(\"âœ… Store table created\")\n",
    "        \n",
    "        # Create checkpoints table\n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS checkpoints (\n",
    "                thread_id TEXT NOT NULL,\n",
    "                checkpoint_ns TEXT NOT NULL DEFAULT '',\n",
    "                checkpoint_id TEXT NOT NULL,\n",
    "                parent_checkpoint_id TEXT,\n",
    "                type TEXT,\n",
    "                checkpoint JSONB NOT NULL,\n",
    "                metadata JSONB NOT NULL DEFAULT '{}',\n",
    "                PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS checkpoints_thread_id_checkpoint_ns_idx \n",
    "            ON checkpoints(thread_id, checkpoint_ns);\n",
    "        \"\"\")\n",
    "        print(\"âœ… Checkpoints table created\")\n",
    "        \n",
    "        # Create writes table\n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS writes (\n",
    "                thread_id TEXT NOT NULL,\n",
    "                checkpoint_ns TEXT NOT NULL DEFAULT '',\n",
    "                checkpoint_id TEXT NOT NULL,\n",
    "                task_id TEXT NOT NULL,\n",
    "                idx INTEGER NOT NULL,\n",
    "                channel TEXT NOT NULL,\n",
    "                type TEXT,\n",
    "                value JSONB,\n",
    "                PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id, task_id, idx)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS writes_thread_id_checkpoint_ns_checkpoint_id_idx \n",
    "            ON writes(thread_id, checkpoint_ns, checkpoint_id);\n",
    "        \"\"\")\n",
    "        print(\"Writes table created\")\n",
    "\n",
    "async def get_pool() -> AsyncConnectionPool:\n",
    "    global _pool\n",
    "    if _pool is None:\n",
    "        _pool = AsyncConnectionPool(\n",
    "            conninfo=POSTGRES_URI,\n",
    "            min_size=1,\n",
    "            max_size=10,\n",
    "            open=False,\n",
    "            timeout=120,\n",
    "            max_waiting=5,\n",
    "            max_lifetime=1800,\n",
    "            max_idle=300,\n",
    "        )\n",
    "        await _pool.open()\n",
    "        async with _pool.connection() as conn:\n",
    "            await conn.execute(\"SELECT 1\")\n",
    "        print(\"âœ… Database pool connected successfully\")\n",
    "    return _pool\n",
    "\n",
    "\n",
    "async def setup_database():\n",
    "    global store, saver\n",
    "    print(\"=== SETUP_DATABASE CALLED ===\")\n",
    "    \n",
    "    # Drop old tables (UNCOMMENT for first run only, then comment out)\n",
    "    # await drop_old_tables_once()\n",
    "    \n",
    "    # Try manual table creation first (more reliable with Supabase pooler)\n",
    "    await create_tables_manually()\n",
    "    \n",
    "    pool = await get_pool()\n",
    "    \n",
    "    # NEW: Create custom user_profiles table with PROPER array syntax\n",
    "    async with pool.connection() as conn:\n",
    "        print(\"Creating custom user_profiles table...\")\n",
    "        \n",
    "        # Drop the table first to recreate with correct schema\n",
    "        await conn.execute(\"DROP TABLE IF EXISTS user_profiles CASCADE\")\n",
    "        \n",
    "        # Create with TEXT[] array type (proper PostgreSQL syntax)\n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE TABLE user_profiles (\n",
    "                business_id VARCHAR(255) NOT NULL,  \n",
    "                user_id VARCHAR(255) NOT NULL, \n",
    "                name VARCHAR(255),\n",
    "                cart JSONB,\n",
    "                address TEXT,\n",
    "                location VARCHAR(255),\n",
    "                human_active BOOLEAN DEFAULT FALSE,\n",
    "                created_at TIMESTAMP DEFAULT NOW(),\n",
    "                updated_at TIMESTAMP DEFAULT NOW(),\n",
    "                PRIMARY KEY (business_id, user_id)  \n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_user_profiles_user_id ON user_profiles(user_id)\n",
    "        \"\"\")\n",
    "        \n",
    "        await conn.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_user_profiles_business_id ON user_profiles(business_id)\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"âœ… Custom user_profiles table created with array column!\")\n",
    "    \n",
    "    # Verify tables exist\n",
    "    async with pool.connection() as conn:\n",
    "        print(\"Verifying tables...\")\n",
    "        \n",
    "        # Check store table\n",
    "        store_result = await conn.execute(\n",
    "            \"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'store')\"\n",
    "        )\n",
    "        store_exists = (await store_result.fetchone())[0]\n",
    "        \n",
    "        # Check checkpoints table\n",
    "        checkpoints_result = await conn.execute(\n",
    "            \"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'checkpoints')\"\n",
    "        )\n",
    "        checkpoints_exists = (await checkpoints_result.fetchone())[0]\n",
    "        \n",
    "        \n",
    "        # Check writes table\n",
    "        writes_result = await conn.execute(\n",
    "            \"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'writes')\"\n",
    "        )\n",
    "        writes_exists = (await writes_result.fetchone())[0]\n",
    "        \n",
    "        # Check user_profiles table\n",
    "        profiles_result = await conn.execute(\n",
    "            \"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'user_profiles')\"\n",
    "        )\n",
    "        user_profiles_exists = (await profiles_result.fetchone())[0]\n",
    "        \n",
    "        print(f\"âœ… Store table exists: {store_exists}\")\n",
    "        print(f\"âœ… Checkpoints table exists: {checkpoints_exists}\")\n",
    "        print(f\"âœ… Writes table exists: {writes_exists}\")\n",
    "        print(f\"âœ… User profiles table exists: {user_profiles_exists}\")\n",
    "        \n",
    "        if not (store_exists and checkpoints_exists and writes_exists and user_profiles_exists):\n",
    "            raise Exception(\"Tables were not created properly!\")\n",
    "        \n",
    "        # Verify cart column exists and is array type\n",
    "        column_check = await conn.execute(\"\"\"\n",
    "            SELECT column_name, data_type \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = 'user_profiles' AND column_name = 'cart'\n",
    "        \"\"\")\n",
    "        cart_col = await column_check.fetchone()\n",
    "        if cart_col:\n",
    "            print(f\"âœ… Cart column: {cart_col[0]} ({cart_col[1]})\")\n",
    "        else:\n",
    "            raise Exception(\"Cart column was not created!\")\n",
    "    \n",
    "    # Create store and saver instances\n",
    "    print(\"Creating store and saver instances...\")\n",
    "    store = AsyncPostgresStore(pool)\n",
    "    saver = AsyncPostgresSaver(pool)\n",
    "    print(\"âœ… Database ready!\")\n",
    "    \n",
    "    return store, saver\n",
    "\n",
    "    \n",
    "saver\n",
    "store\n",
    "\n",
    "\n",
    "async def chatbot(state: MessagesState, config: RunnableConfig):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    global store\n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    business_id = config[\"configurable\"][\"business_id\"]\n",
    "\n",
    "    # Consistent namespace order\n",
    "    namespace = (\"profile\", business_id, user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "\n",
    "    # Extract the actual memory content if it exists\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MSG_PROMPT.format(user_profile=existing_memory_content)\n",
    "    \n",
    "    # FIXED: Pass both system message AND conversation history\n",
    "    response = await model.bind_tools([CustomerAction]).ainvoke(\n",
    "        [SystemMessage(content=system_msg)] + state[\"messages\"]\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def write_memory(state: MessagesState, config: RunnableConfig):\n",
    "    \"\"\"Extract and save profile information.\"\"\"\n",
    "    global store\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    business_id = config[\"configurable\"][\"business_id\"]\n",
    "    \n",
    "    namespace = (\"profile\", business_id, user_id)\n",
    "    \n",
    "    existing_items = await store.asearch(namespace)\n",
    "    tool_name = \"Profile\"\n",
    "    existing_memories = (\n",
    "        [(item.key, tool_name, item.value) for item in existing_items]\n",
    "        if existing_items else None\n",
    "    )\n",
    "    \n",
    "    conversation_history = [\n",
    "        msg for msg in state[\"messages\"]\n",
    "        if not (hasattr(msg, 'type') and msg.type == 'tool')\n",
    "    ]\n",
    "    \n",
    "    TRUSTCALL_INSTRUCTION_FORMATTED = TRUSTCALL_INSTRUCTION.format(memory=existing_memories)\n",
    "    updated_messages = list(merge_message_runs(\n",
    "        [SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + conversation_history\n",
    "    ))\n",
    "    \n",
    "    result = await profile_extractor.ainvoke({\n",
    "        \"messages\": updated_messages,\n",
    "        \"existing\": existing_memories\n",
    "    })\n",
    "    \n",
    "    profile_data: Profile = result['responses'][0]\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"DEBUG: Extracted profile data:\")\n",
    "    print(f\"Business ID: {business_id}\")\n",
    "    print(f\"User ID: {user_id}\")\n",
    "    print(f\"Name: {profile_data.name}\")\n",
    "    print(f\"Location: {profile_data.location}\")\n",
    "    print(f\"Address: {profile_data.address}\")\n",
    "    print(f\"Cart: {profile_data.cart}\")\n",
    "    print(f\"Human Active: {profile_data.human_active}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Save to LangGraph store\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        await store.aput(\n",
    "            namespace,\n",
    "            \"user_memory\",\n",
    "            r.model_dump(mode=\"json\"),\n",
    "        )\n",
    "    \n",
    "    # Convert cart to JSON string before inserting\n",
    "    pool = await get_pool()\n",
    "    async with pool.connection() as conn:\n",
    "        # Convert cart to JSON - handle both list and dict formats\n",
    "        cart_json = json.dumps(profile_data.cart) if profile_data.cart is not None else None\n",
    "        \n",
    "        await conn.execute(\n",
    "            '''\n",
    "            INSERT INTO user_profiles (business_id, user_id, name, cart, address, location, human_active, updated_at)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())\n",
    "            ON CONFLICT (business_id, user_id) DO UPDATE SET\n",
    "                name = COALESCE(EXCLUDED.name, user_profiles.name),\n",
    "                cart = COALESCE(EXCLUDED.cart, user_profiles.cart),\n",
    "                address = COALESCE(EXCLUDED.address, user_profiles.address),\n",
    "                location = COALESCE(EXCLUDED.location, user_profiles.location),\n",
    "                human_active = COALESCE(EXCLUDED.human_active, user_profiles.human_active),\n",
    "                updated_at = NOW()\n",
    "            ''',\n",
    "            (\n",
    "                business_id,\n",
    "                user_id, \n",
    "                profile_data.name, \n",
    "                cart_json,  # âœ… Now it's a JSON string\n",
    "                profile_data.address,\n",
    "                profile_data.location,\n",
    "                profile_data.human_active\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Return tool message\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Profile information saved successfully\",\n",
    "            \"tool_call_id\": tool_calls[0]['id']\n",
    "        }]\n",
    "    }\n",
    "\n",
    "\n",
    "async def check_address_and_finalize(state: MessagesState, config: RunnableConfig):\n",
    "    \"\"\"Check if user has address, ask if not, or prepare for escalation.\"\"\"\n",
    "    global store\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    business_id = config[\"configurable\"][\"business_id\"]\n",
    "    \n",
    "    # FIXED: Consistent namespace\n",
    "    namespace = (\"profile\", business_id, user_id)\n",
    "    \n",
    "    existing_memory = await store.aget(namespace, \"user_memory\")\n",
    "    \n",
    "    # Check for address in profile\n",
    "    has_address = False\n",
    "    if existing_memory:\n",
    "        profile_data = existing_memory.value\n",
    "        has_address = profile_data.get('address') is not None\n",
    "    \n",
    "    if not has_address:\n",
    "        # Ask for address\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"User is ready to order but no delivery address on file. Please ask for their delivery address.\",\n",
    "                \"tool_call_id\": tool_calls[0]['id']\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        # Has address - ready to escalate\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"User has address on file and is ready to complete order. Escalating to operator.\",\n",
    "                \"tool_call_id\": tool_calls[0]['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    \n",
    "async def add_to_cart(state: MessagesState, config: RunnableConfig):\n",
    "    \"\"\"Add items to the user's cart with quantity support.\"\"\"\n",
    "    global store\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    business_id = config[\"configurable\"][\"business_id\"]\n",
    "    \n",
    "    # Configuration\n",
    "    MAX_CART_SIZE = 50\n",
    "    MAX_ITEM_QUANTITY = 99\n",
    "    \n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call.get('args', {})\n",
    "    \n",
    "    cart_items = args.get('cart_items', [])\n",
    "    \n",
    "    if not isinstance(cart_items, list):\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Invalid cart items format. Expected a list.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    if not cart_items:\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"No items specified to add to cart.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        pool = await get_pool()\n",
    "        \n",
    "        async with pool.connection() as conn:\n",
    "            # Get current cart\n",
    "            result = await conn.execute(\n",
    "                \"SELECT cart FROM user_profiles WHERE business_id = %s AND user_id = %s\",\n",
    "                (business_id, user_id)\n",
    "            )\n",
    "            \n",
    "            row = await result.fetchone()\n",
    "            \n",
    "            # Parse cart from JSON if it exists\n",
    "            if row and row[0]:\n",
    "                import json\n",
    "                current_cart = json.loads(row[0]) if isinstance(row[0], str) else row[0]\n",
    "            else:\n",
    "                current_cart = {}\n",
    "            \n",
    "            # Convert old list format to dict format if needed\n",
    "            if isinstance(current_cart, list):\n",
    "                current_cart = {item: 1 for item in current_cart}\n",
    "            \n",
    "            # Process new items\n",
    "            added_items = []\n",
    "            updated_items = []\n",
    "            quantity_limited_items = []\n",
    "            \n",
    "            for item in cart_items:\n",
    "                # Now we only expect dict format\n",
    "                if isinstance(item, dict):\n",
    "                    item_name = item.get('item')\n",
    "                    quantity = item.get('quantity', 1)\n",
    "                else:\n",
    "                    # Fallback for safety\n",
    "                    continue\n",
    "                \n",
    "                if not item_name:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate new quantity\n",
    "                current_qty = current_cart.get(item_name, 0)\n",
    "                new_qty = current_qty + quantity\n",
    "                \n",
    "                # Apply quantity limit\n",
    "                if new_qty > MAX_ITEM_QUANTITY:\n",
    "                    new_qty = MAX_ITEM_QUANTITY\n",
    "                    quantity_limited_items.append(item_name)\n",
    "                \n",
    "                # Update cart\n",
    "                if current_qty == 0:\n",
    "                    added_items.append(f\"{item_name} (x{new_qty})\")\n",
    "                else:\n",
    "                    updated_items.append(f\"{item_name} ({current_qty} â†’ {new_qty})\")\n",
    "                \n",
    "                current_cart[item_name] = new_qty\n",
    "            \n",
    "            # Check unique items limit\n",
    "            if len(current_cart) > MAX_CART_SIZE:\n",
    "                return {\n",
    "                    \"messages\": [{\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": f\"Cannot add items. Cart limit is {MAX_CART_SIZE} unique items.\",\n",
    "                        \"tool_call_id\": tool_call['id']\n",
    "                    }]\n",
    "                }\n",
    "            \n",
    "            # Calculate total items\n",
    "            total_items = sum(current_cart.values())\n",
    "            \n",
    "            # Convert to JSON before inserting\n",
    "            import json\n",
    "            cart_json = json.dumps(current_cart)\n",
    "            \n",
    "            # Update database\n",
    "            await conn.execute(\n",
    "                '''\n",
    "                INSERT INTO user_profiles (business_id, user_id, cart, updated_at)\n",
    "                VALUES (%s, %s, %s, NOW())\n",
    "                ON CONFLICT (business_id, user_id) DO UPDATE SET\n",
    "                    cart = EXCLUDED.cart,\n",
    "                    updated_at = NOW()\n",
    "                ''',\n",
    "                (business_id, user_id, cart_json)\n",
    "            )\n",
    "            \n",
    "            # Update store\n",
    "            try:\n",
    "                namespace = (\"profile\", business_id, user_id)\n",
    "                existing_memory = await store.aget(namespace, \"user_memory\")\n",
    "                \n",
    "                if existing_memory:\n",
    "                    memory_data = existing_memory.value\n",
    "                    memory_data['cart'] = current_cart\n",
    "                else:\n",
    "                    memory_data = {'cart': current_cart}\n",
    "                \n",
    "                await store.aput(namespace, \"user_memory\", memory_data)\n",
    "            except Exception as store_error:\n",
    "                logger.warning(f\"Failed to update store: {store_error}\")\n",
    "        \n",
    "        # Format success message\n",
    "        message_parts = []\n",
    "        \n",
    "        if added_items:\n",
    "            message_parts.append(f\"Added: {', '.join(added_items)}\")\n",
    "        \n",
    "        if updated_items:\n",
    "            message_parts.append(f\"Updated: {', '.join(updated_items)}\")\n",
    "        \n",
    "        message = \". \".join(message_parts) if message_parts else \"Cart updated\"\n",
    "        message += f\". Cart now has {len(current_cart)} unique item(s) ({total_items} total items).\"\n",
    "        \n",
    "        if quantity_limited_items:\n",
    "            message += f\" Note: {', '.join(quantity_limited_items)} reached maximum quantity of {MAX_ITEM_QUANTITY}.\"\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": message,\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to add to cart: {e}\")\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Failed to add items to cart. Please try again.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "\n",
    "\n",
    "async def remove_cart_item(state: MessagesState, config: RunnableConfig):\n",
    "    \"\"\"Remove items from the user's cart with quantity support.\"\"\"\n",
    "    global store\n",
    "\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    business_id = config[\"configurable\"][\"business_id\"]\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call.get(\"args\", {})\n",
    "\n",
    "    items_to_remove = args.get(\"items_to_remove\", [])\n",
    "\n",
    "    \n",
    "    if not isinstance(items_to_remove, list):\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Invalid items format. Expected a list.\",\n",
    "                \"tool_call_id\": tool_call[\"id\"]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    if not items_to_remove:\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"No items specified to remove from cart.\",\n",
    "                \"tool_call_id\": tool_call[\"id\"]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        pool = await get_pool()\n",
    "\n",
    "        async with pool.connection() as conn:\n",
    "            # ---- Load cart ----\n",
    "            result = await conn.execute(\n",
    "                \"SELECT cart FROM user_profiles WHERE business_id = %s AND user_id = %s\",\n",
    "                (business_id, user_id)\n",
    "            )\n",
    "            row = await result.fetchone()\n",
    "\n",
    "            raw_cart = row[0] if row else None\n",
    "\n",
    "            # Normalize cart format â†’ always a dict\n",
    "            if isinstance(raw_cart, str):\n",
    "                current_cart = json.loads(raw_cart)\n",
    "            elif isinstance(raw_cart, list):\n",
    "                current_cart = {item: 1 for item in raw_cart}\n",
    "            else:\n",
    "                current_cart = raw_cart or {}\n",
    "\n",
    "            if not current_cart:\n",
    "                return {\n",
    "                    \"messages\": [{\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": \"Your cart is empty. Nothing to remove.\",\n",
    "                        \"tool_call_id\": tool_call[\"id\"]\n",
    "                    }]\n",
    "                }\n",
    "\n",
    "            removed_items = []\n",
    "            updated_items = []\n",
    "            not_found_items = []\n",
    "\n",
    "            # ---- Process removals ----\n",
    "            for item in items_to_remove:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "\n",
    "                item_name = item.get(\"item\")\n",
    "                quantity_to_remove = item.get(\"quantity\", 1)\n",
    "\n",
    "                if not item_name:\n",
    "                    continue\n",
    "\n",
    "                if item_name not in current_cart:\n",
    "                    not_found_items.append(item_name)\n",
    "                    continue\n",
    "\n",
    "                current_qty = current_cart[item_name]\n",
    "\n",
    "                if quantity_to_remove >= current_qty:\n",
    "                    removed_items.append(f\"{item_name} (all {current_qty} removed)\")\n",
    "                    del current_cart[item_name]\n",
    "                else:\n",
    "                    new_qty = current_qty - quantity_to_remove\n",
    "                    current_cart[item_name] = new_qty\n",
    "                    updated_items.append(f\"{item_name} ({current_qty} â†’ {new_qty})\")\n",
    "\n",
    "            # ---- No changes? ----\n",
    "            if not removed_items and not updated_items:\n",
    "                available_items = [f\"{item} (x{qty})\" for item, qty in current_cart.items()]\n",
    "                return {\n",
    "                    \"messages\": [{\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": (\n",
    "                            f\"Could not find items to remove: {', '.join(not_found_items)}. \"\n",
    "                            f\"Your cart contains: {', '.join(available_items)}.\"\n",
    "                        ),\n",
    "                        \"tool_call_id\": tool_call[\"id\"]\n",
    "                    }]\n",
    "                }\n",
    "\n",
    "            # ---- Save updated cart ----\n",
    "            await conn.execute(\n",
    "                \"\"\"\n",
    "                UPDATE user_profiles\n",
    "                SET cart = %s, updated_at = NOW()\n",
    "                WHERE business_id = %s AND user_id = %s\n",
    "                \"\"\",\n",
    "                (json.dumps(current_cart), business_id, user_id)\n",
    "            )\n",
    "\n",
    "            # ---- Update memory store (optional) ----\n",
    "            try:\n",
    "                namespace = (\"profile\", business_id, user_id)\n",
    "                existing_memory = await store.aget(namespace, \"user_memory\")\n",
    "                memory_data = existing_memory.value if existing_memory else {}\n",
    "                memory_data[\"cart\"] = current_cart\n",
    "                await store.aput(namespace, \"user_memory\", memory_data)\n",
    "            except Exception as store_error:\n",
    "                logger.warning(f\"Failed to update store, but database was updated: {store_error}\")\n",
    "\n",
    "        # ---- Build success message ----\n",
    "        parts = []\n",
    "        if removed_items:\n",
    "            parts.append(f\"Removed completely: {', '.join(removed_items)}\")\n",
    "        if updated_items:\n",
    "            parts.append(f\"Reduced quantity: {', '.join(updated_items)}\")\n",
    "\n",
    "        message = \". \".join(parts) if parts else \"Cart updated\"\n",
    "\n",
    "        if current_cart:\n",
    "            remaining_items = [f\"{item} (x{qty})\" for item, qty in current_cart.items()]\n",
    "            total_remaining = sum(current_cart.values())\n",
    "            message += (\n",
    "                f\". Cart now has {len(current_cart)} unique item(s) ({total_remaining} total): \"\n",
    "                f\"{', '.join(remaining_items)}.\"\n",
    "            )\n",
    "        else:\n",
    "            message += \". Your cart is now empty.\"\n",
    "\n",
    "        if not_found_items:\n",
    "            message += f\" Note: Could not find: {', '.join(not_found_items)}.\"\n",
    "\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": message,\n",
    "                \"tool_call_id\": tool_call[\"id\"]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to remove from cart: {e}\")\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Failed to remove items from cart. Please try again.\",\n",
    "                \"tool_call_id\": tool_call[\"id\"]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "## RAG\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# KB1 Setup\n",
    "KB1_DOC_PATH = r\"C:\\Users\\HP\\Desktop\\WAHA\\synthetic_restaurant_menu_10000.csv\"\n",
    "\n",
    "# \n",
    "_vector_store_cache = {}\n",
    "_retriever_cache = {}\n",
    "_file_hash_cache = {}\n",
    "_embeddings_cache = None  \n",
    "_documents_loaded = {} \n",
    "\n",
    "\n",
    "async def get_file_hash(path):\n",
    "    \"\"\"Compute a hash of the file contents to detect changes.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    def _read_file():\n",
    "        with open(path, \"rb\") as f:\n",
    "            return hasher.update(f.read())\n",
    "    \n",
    "    await loop.run_in_executor(None, _read_file)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "@retry_with_backoff(\n",
    "    max_retries=3,\n",
    "    initial_delay=1.0,\n",
    "    jitter=\"full\",\n",
    "    exceptions=(APIError, RateLimitError, Timeout)\n",
    ")\n",
    "async def initialize_rag(business_id=None, doc_path=None):\n",
    "    \"\"\"Initialize RAG system with Supabase pgvector. Call once at startup per business.\"\"\"\n",
    "    global _vector_store_cache, _retriever_cache, _file_hash_cache, _embeddings_cache, _documents_loaded\n",
    "    \n",
    "    # CRITICAL: Validate business_id\n",
    "    if not business_id:\n",
    "        raise ValueError(\"business_id is required for RAG initialization\")\n",
    "    \n",
    "    # FIXED: Check if already initialized for THIS business\n",
    "    if business_id in _retriever_cache and _documents_loaded.get(business_id, False):\n",
    "        print(f\"RAG already initialized for business {business_id}\")\n",
    "        return _retriever_cache[business_id]\n",
    "    \n",
    "    document_path = doc_path or KB1_DOC_PATH\n",
    "\n",
    "    try:\n",
    "        # Initialize embeddings once and store in global cache (shared across businesses)\n",
    "        if _embeddings_cache is None:\n",
    "            print(\"Initializing embeddings model...\")\n",
    "            _embeddings_cache = HuggingFaceEmbeddings(\n",
    "                model_name=EMBEDDING_MODEL_NAME,\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "        \n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # âœ… FIXED: Use business_id in collection name\n",
    "        collection_name = f\"business_{business_id}_menu\"\n",
    "        print(f\"Connecting to Supabase pgvector for collection: {collection_name}\")\n",
    "        \n",
    "        # Initialize PGVector connection to Supabase (use pooler)\n",
    "        vector_store = PGVector(\n",
    "            embeddings=_embeddings_cache,\n",
    "            collection_name=collection_name,\n",
    "            connection=POSTGRES_URI_POOLER,  # Use pooler for PGVector\n",
    "            use_jsonb=True,\n",
    "        )\n",
    "        \n",
    "        # Check if documents already exist in the collection\n",
    "        try:\n",
    "            print(f\"Checking for existing documents in {collection_name}...\")\n",
    "            # Try a simple search to see if collection has data\n",
    "            test_results = await loop.run_in_executor(\n",
    "                None,\n",
    "                lambda: vector_store.similarity_search(\"test\", k=1)\n",
    "            )\n",
    "            \n",
    "            if test_results and len(test_results) > 0:\n",
    "                # Check if file has changed\n",
    "                current_hash = await get_file_hash(document_path)\n",
    "                \n",
    "                print(f\"âœ… Found existing documents in Supabase for business {business_id}\")\n",
    "                _file_hash_cache[business_id] = current_hash\n",
    "                _documents_loaded[business_id] = True\n",
    "                _vector_store_cache[business_id] = vector_store\n",
    "                _retriever_cache[business_id] = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "                return _retriever_cache[business_id]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"No existing documents found for business {business_id}: {e}\")\n",
    "            print(\"Will load documents from file...\")\n",
    "        \n",
    "        # Load and process documents\n",
    "        print(f\"Building knowledge base from file for business {business_id}...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "        all_documents = []\n",
    "        \n",
    "        # Load from document\n",
    "        if document_path and os.path.exists(document_path):\n",
    "            print(f\"Processing {document_path}\")\n",
    "            if document_path.lower().endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(document_path)\n",
    "            elif document_path.lower().endswith(\".docx\"):\n",
    "                loader = Docx2txtLoader(document_path)\n",
    "            elif document_path.lower().endswith(\".csv\"):\n",
    "                loader = CSVLoader(file_path=document_path, encoding=\"utf-8-sig\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {document_path}\")\n",
    "            \n",
    "            docs = await loop.run_in_executor(None, loader.load)\n",
    "            doc_chunks = await loop.run_in_executor(None, splitter.split_documents, docs)\n",
    "            all_documents.extend(doc_chunks)\n",
    "            print(f\"Loaded {len(doc_chunks)} chunks from {document_path}\")\n",
    "\n",
    "        if not all_documents:\n",
    "            raise ValueError(\"No documents were loaded\")\n",
    "\n",
    "        # Store file hash per business\n",
    "        _file_hash_cache[business_id] = await get_file_hash(document_path)\n",
    "        \n",
    "        print(f\"Adding {len(all_documents)} document chunks to Supabase for business {business_id}...\")\n",
    "        print(\"â³ This may take a few minutes for large datasets...\")\n",
    "        \n",
    "        # Add documents to PGVector in batches for better performance\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(all_documents), batch_size):\n",
    "            batch = all_documents[i:i+batch_size]\n",
    "            await loop.run_in_executor(\n",
    "                None,\n",
    "                lambda b=batch: vector_store.add_documents(b)\n",
    "            )\n",
    "            print(f\"  Progress: {min(i+batch_size, len(all_documents))}/{len(all_documents)} documents added\")\n",
    "        \n",
    "        _documents_loaded[business_id] = True\n",
    "        _vector_store_cache[business_id] = vector_store\n",
    "        _retriever_cache[business_id] = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "        print(f\"âœ… Setup complete - {len(all_documents)} documents indexed in Supabase for business {business_id}!\")\n",
    "        \n",
    "        return _retriever_cache[business_id]\n",
    "      \n",
    "    except Exception as e:\n",
    "        logger.error(f\"RAG initialization failed for business {business_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "async def refresh_rag(business_id=None, doc_path=None):\n",
    "    \"\"\"Refresh RAG when file changes detected - clears and reloads documents for a specific business.\"\"\"\n",
    "    global _vector_store_cache, _retriever_cache, _file_hash_cache, _embeddings_cache, _documents_loaded\n",
    "    \n",
    "    if not business_id:\n",
    "        raise ValueError(\"business_id is required for RAG refresh\")\n",
    "    \n",
    "    print(f\"ðŸ”„ Refreshing knowledge base for business {business_id}...\")\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Delete existing collection in Supabase for this business\n",
    "    if business_id in _vector_store_cache:\n",
    "        try:\n",
    "            print(f\"Clearing existing documents from Supabase for business {business_id}...\")\n",
    "            await loop.run_in_executor(\n",
    "                None,\n",
    "                lambda: _vector_store_cache[business_id].delete_collection()\n",
    "            )\n",
    "            print(\"âœ… Collection cleared\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing collection: {e}\")\n",
    "    \n",
    "    # Clear cache for this business only\n",
    "    _vector_store_cache.pop(business_id, None)\n",
    "    _retriever_cache.pop(business_id, None)\n",
    "    _file_hash_cache.pop(business_id, None)\n",
    "    _documents_loaded.pop(business_id, None)\n",
    "    # Keep embeddings cache to avoid reloading model\n",
    "    \n",
    "    # Reinitialize for this business\n",
    "    retriever = await initialize_rag(business_id=business_id, doc_path=doc_path)\n",
    "    print(f\"âœ… Knowledge base refreshed for business {business_id}!\")\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "# File change handler\n",
    "class KBUpdateHandler(FileSystemEventHandler):\n",
    "    def __init__(self, business_id):\n",
    "        self.business_id = business_id\n",
    "        super().__init__()\n",
    "    \n",
    "    def on_modified(self, event):\n",
    "        if event.src_path == os.path.abspath(KB1_DOC_PATH):\n",
    "            print(f\"ðŸ“ Detected change in file: {event.src_path} for business {self.business_id}\")\n",
    "            try:\n",
    "                asyncio.create_task(refresh_rag(business_id=self.business_id))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error refreshing: {e}\")\n",
    "\n",
    "\n",
    "def start_file_monitoring(business_id):\n",
    "    \"\"\"Start watching file for changes for a specific business.\"\"\"\n",
    "    event_handler = KBUpdateHandler(business_id)\n",
    "    observer = Observer()\n",
    "    \n",
    "    kb_dir = os.path.dirname(os.path.abspath(KB1_DOC_PATH))\n",
    "    observer.schedule(event_handler, kb_dir, recursive=False)\n",
    "    \n",
    "    observer.start()\n",
    "    print(f\"File monitoring started for business {business_id}...\")\n",
    "    return observer\n",
    "\n",
    "_init_lock={}\n",
    "_locks_lock=Lock()\n",
    "\n",
    "def get_init_lock(business_id):\n",
    "    \"\"\" Get or create lock a particular business ID\"\"\"\n",
    "    with _locks_lock:\n",
    "        if business_id not in _init_lock:\n",
    "            _init_lock[business_id]= asyncio.Lock()\n",
    "        return _init_lock[business_id]\n",
    "        \n",
    "@retry_with_backoff(\n",
    "    max_retries=3,\n",
    "    initial_delay=1.0,\n",
    "    jitter=\"full\",\n",
    "    exceptions=(ConnectionError, OSError)\n",
    ")\n",
    "async def rag_search(state: MessagesState, config: RunnableConfig):\n",
    "    \"\"\"Perform RAG search on knowledge base.\"\"\"\n",
    "    business_id = config[\"configurable\"][\"business_id\"]\n",
    "    \n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call.get('args', {})\n",
    "    search_query = args.get('search_query', '')\n",
    "    \n",
    "    if not search_query:\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"No search query provided.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Initialize (has its own retry for APIError, RateLimitError, Timeout)\n",
    "        init_lock = get_init_lock(business_id)\n",
    "        async with init_lock:\n",
    "            if business_id not in _retriever_cache:\n",
    "                await initialize_rag(business_id=business_id)\n",
    "        \n",
    "        retriever = _retriever_cache.get(business_id)\n",
    "        \n",
    "        if not retriever:\n",
    "            return {\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": \"Knowledge base not initialized. Please contact support.\",\n",
    "                    \"tool_call_id\": tool_call['id']\n",
    "                }]\n",
    "            }\n",
    "        \n",
    "        # This part might benefit from retry (connection issues during search)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        relevant_docs = await loop.run_in_executor(\n",
    "            None, \n",
    "            lambda: retriever.invoke(search_query)\n",
    "        )\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return {\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": \"I couldn't find any information about that in the knowledge base.\",\n",
    "                    \"tool_call_id\": tool_call['id']\n",
    "                }]\n",
    "            }\n",
    "        \n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": context,\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    except (APIError, RateLimitError, Timeout) as e:\n",
    "        # These already retried in initialize_rag, don't retry again\n",
    "        logger.error(f\"RAG initialization failed after retries for business {business_id}: {e}\")\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Sorry, knowledge base initialization failed. Please try again later.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Other errors get logged and returned (or let retry wrapper handle if it's ConnectionError/OSError)\n",
    "        logger.error(f\"RAG search failed for business {business_id}: {e}\", exc_info=True)\n",
    "        return {\n",
    "            \"messages\": [{\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"Sorry, I encountered an error searching the knowledge base. Please try again.\",\n",
    "                \"tool_call_id\": tool_call['id']\n",
    "            }]\n",
    "        }\n",
    "\n",
    "def route_customer_action(state: MessagesState) -> str:\n",
    "    \"\"\"Route to the appropriate action node based on priority.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if not (hasattr(last_message, 'tool_calls') and last_message.tool_calls):\n",
    "        return \"__end__\"\n",
    "    \n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    args = tool_call.get('args', {})\n",
    "    \n",
    "    \n",
    "    if args.get('ready_to_order'):\n",
    "        return \"check_address_and_finalize\"\n",
    "    \n",
    "    if args.get('update_profile'):\n",
    "        return \"write_memory\"\n",
    "    \n",
    "    if args.get('search_menu'):\n",
    "        return \"rag_search\"\n",
    "    \n",
    "    if args.get('add_to_cart'):\n",
    "        return \"add_to_cart\"\n",
    "    \n",
    "    if args.get('items_to_remove'):\n",
    "        return \"remove_cart_item\"\n",
    "    \n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_node(\"rag_search\", rag_search)\n",
    "builder.add_node(\"check_address_and_finalize\", check_address_and_finalize)\n",
    "builder.add_node(\"add_to_cart\", add_to_cart)\n",
    "builder.add_node(\"remove_cart_item\", remove_cart_item)\n",
    "\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_customer_action,\n",
    "    {\n",
    "        \"write_memory\": \"write_memory\",\n",
    "        \"rag_search\": \"rag_search\",\n",
    "        \"check_address_and_finalize\": \"check_address_and_finalize\",\n",
    "        \"add_to_cart\": \"add_to_cart\",\n",
    "        \"remove_cart_item\": \"remove_cart_item\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# All actions loop back to chatbot\n",
    "builder.add_edge(\"write_memory\", \"chatbot\")\n",
    "builder.add_edge(\"rag_search\", \"chatbot\")\n",
    "builder.add_edge(\"check_address_and_finalize\", \"chatbot\")\n",
    "builder.add_edge(\"add_to_cart\", \"chatbot\")\n",
    "builder.add_edge(\"remove_cart_item\", \"chatbot\")\n",
    "\n",
    "store, saver = await setup_database()\n",
    "graph = builder.compile(checkpointer=saver, store=store)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install structlog bleach tenacity circuitbreaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d18493",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_id= \"123ABC\"\n",
    "user_id= \"@030\"\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": f\"user_{user_id}_business_{business_id}\",\n",
    "        \"user_id\": user_id,\n",
    "        \"business_id\": business_id\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: Setting name and location (NOT full address)\")\n",
    "print(\"=\"*60)\n",
    "input_messages = [HumanMessage(content=\"My name is Kanny.\")]\n",
    "async for chunk in graph.astream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4d84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: Providing FULL delivery address\")\n",
    "print(\"=\"*60)\n",
    "input_messages = [HumanMessage(content=\"My delivery address is 55 Allen Ave, Ikeja, Lagos State, Nigeria\")]\n",
    "async for chunk in graph.astream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"What did I last add to cart?\")]\n",
    "async for chunk in graph.astream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"I need 2 sandwiches and 1 more burger\")]\n",
    "async for chunk in graph.astream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"How many items do I have in my cart?.\")]\n",
    "async for chunk in graph.astream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"remove 1 burger from my cart\")]\n",
    "async for chunk in graph.astream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f0aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2769a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "from typing import Any, Callable, Tuple\n",
    "from functools import wraps\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def retry_with_backoff(\n",
    "        max_retries: int = 5,\n",
    "        initial_delay: float = 1.0,\n",
    "        exponential_base: float = 2.0,\n",
    "        jitter: str = \"full\",       # \"none\", \"full\", \"equal\", \"decorrelated\"\n",
    "        exceptions: Tuple = (Exception,)\n",
    "):\n",
    "    \"\"\"\n",
    "    Retry decorator with exponential backoff and optional jitter.\n",
    "\n",
    "    Jitter modes:\n",
    "        - none: no jitter\n",
    "        - full: random(0, delay)\n",
    "        - equal: delay/2 + random(0, delay/2)\n",
    "        - decorrelated: random(initial_delay, delay * 3)\n",
    "    \"\"\"\n",
    "\n",
    "    def add_jitter(delay: float) -> float:\n",
    "        if jitter == \"none\":\n",
    "            return delay\n",
    "        if jitter == \"full\":\n",
    "            return random.uniform(0, delay)\n",
    "        if jitter == \"equal\":\n",
    "            return delay / 2 + random.uniform(0, delay / 2)\n",
    "        if jitter == \"decorrelated\":\n",
    "            return random.uniform(initial_delay, delay * 3)\n",
    "        return delay\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        \n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs) -> Any:\n",
    "            delay = initial_delay\n",
    "            last_exception = None\n",
    "\n",
    "            for attempt in range(1, max_retries + 2):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except exceptions as e:\n",
    "                    last_exception = e\n",
    "\n",
    "                    if attempt > max_retries:\n",
    "                        logger.error(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                        raise\n",
    "\n",
    "                    sleep_time = add_jitter(delay)\n",
    "                    logger.warning(\n",
    "                        f\"Attempt {attempt} failed: {e}. \"\n",
    "                        f\"Retrying in {sleep_time:.2f}s\"\n",
    "                    )\n",
    "\n",
    "                    time.sleep(sleep_time)\n",
    "                    delay *= exponential_base\n",
    "\n",
    "            raise last_exception\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "\n",
    "from openai import OpenAI, APIError, RateLimitError, Timeout\n",
    "\n",
    "@retry_with_backoff(\n",
    "    max_retries=3,\n",
    "    initial_delay=1.0,\n",
    "    jitter=\"full\",\n",
    "    exceptions=(APIError, RateLimitError, Timeout)\n",
    ")\n",
    "\n",
    "def call_llm_with_retry(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    \"\"\"Call LLM with automatic retry on transient errors.\"\"\"\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        timeout=30.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "def search_engine(query:str):\n",
    "    \"\"\" This is a simple seacrh engine app\"\"\"\n",
    "\n",
    "    seacrh= DuckDuckGoSearchRun()\n",
    "    llm = ChatOpenAI(model=\"GPT-4\", temperature=0)\n",
    "\n",
    "    tools=[\n",
    "        Tool(\n",
    "            name=\"web search\",\n",
    "            func=seacrh.run,\n",
    "            description=\"Search the internet for current information. Input should be a search query.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
